{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Impossible to import Faiss library!! Switching to standard nearest neighbors search implementation, this will be significantly slower.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "from Dictionary import Dictionary\n",
    "from Discriminator import Discriminator\n",
    "from TrainModel import TrainModel\n",
    "from Evaluator import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "#default & settings\n",
    "\n",
    "class parameters(object):  \n",
    "    def __init__(self):\n",
    "    \n",
    "        self.useGPU = True        # Use GPU or not\n",
    "        self.max_n_embed = 1000        # Max number of embeddings loaded, negative means load all\n",
    "        self.n_epoch_adv = 5      # Number of epochs for adversial training\n",
    "        self.epoch_size_adv = 1000 # Iterations per epoch for adversial training\n",
    "        self.batch_size_adv = 32   # Batch size for adversial training\n",
    "        self.dis_steps_adv = 5     # Discriminator steps\n",
    "        self.feedback_coeff = 1    # Discriminator loss feedback coefficient\n",
    "        self.map_beta = 0.001      # Beta for orthogonalization\n",
    "        self.csls_k = 10           # k nearest neighbors in CSLS\n",
    "\n",
    "param_list = parameters()\n",
    "\n",
    "# test purpose\n",
    "if param_list.useGPU:\n",
    "    print param_list.n_epoch_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(source_embedding_path, target_embedding_path, maxCount = 1e10):\n",
    "    # Load embeddings    \n",
    "    # read txt embeddings for English(2519370 words with 300 dim) and Chinese(332647 words with 300 dim)\n",
    "    \n",
    "    word2id = {}     # e.g. u'\\u5e74 = year\n",
    "    vectors = []\n",
    "    count = 0\n",
    "    with io.open(target_embedding_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            #print i,line\n",
    "            #print i\n",
    "            \n",
    "            if param_list.max_n_embed>=0 and count>=param_list.max_n_embed:\n",
    "                break\n",
    "            count += 1\n",
    "            if i == 0:\n",
    "                split = line.split()\n",
    "            else:\n",
    "                word, vect = line.rstrip().split(' ', 1) #stripe space from end\n",
    "                #print word #real chars\n",
    "\n",
    "                vect = np.fromstring(vect, sep=' ')\n",
    "\n",
    "                if np.linalg.norm(vect) == 0:  # avoid to have null embeddings\n",
    "                    vect[0] = 0.001 #add a little amount...\n",
    "                \n",
    "                word2id[word] = count-2\n",
    "                vectors.append(vect[None])\n",
    "    \n",
    "#     print len(vectors[0]),word2id\n",
    "    print \"Finished loading\", count, \"words...\"\n",
    "    id2word = {v: k for k, v in word2id.items()}  #reverse of word2id\n",
    "    dic = Dictionary(id2word, word2id, \"zh\")\n",
    "    #print \"len is\",dic.__len__()\n",
    "    embeddings = np.concatenate(vectors, 0)\n",
    "    embeddings = torch.from_numpy(embeddings).float()\n",
    "    return dic, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading 1000 words...\n",
      "Finished loading 1000 words...\n"
     ]
    }
   ],
   "source": [
    "# load source embedding\n",
    "source_embedding_path = \"data/wiki.en.vec\"\n",
    "target_embedding_path = \"data/wiki.zh.vec\"\n",
    "src_dic, _src_emb = load_embeddings(source_embedding_path,source_embedding_path, 100)\n",
    "src_emb = nn.Embedding(len(src_dic), 300, sparse=True) #dim is set to 300..\n",
    "\n",
    "# load target embedding\n",
    "tgt_dic, _tgt_emb = load_embeddings(target_embedding_path,target_embedding_path, 100)\n",
    "tgt_emb = nn.Embedding(len(tgt_dic), 300, sparse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping\n",
    "mapping = nn.Linear(300, 300, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.1)\n",
       "  (1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "  (2): LeakyReLU(0.2)\n",
       "  (3): Dropout(p=0)\n",
       "  (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (5): LeakyReLU(0.2)\n",
       "  (6): Dropout(p=0)\n",
       "  (7): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  (8): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu\n",
    "if param_list.useGPU:\n",
    "    src_emb.cuda()\n",
    "    tgt_emb.cuda()\n",
    "    mapping.cuda()\n",
    "    discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not normalize embeddings\n",
    "# params.src_mean = normalize_embeddings(src_emb.weight.data, \"\")\n",
    "# params.tgt_mean = normalize_embeddings(tgt_emb.weight.data, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we have these four core part cuda: src_emb.cuda(), tgt_emb.cuda(), mapping.cuda(), discriminator.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TrainModel(src_emb, tgt_emb, mapping, discriminator, src_dic, tgt_dic, 'sgd', 0.1, param_list)\n",
    "#trainer = TrainModel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Linear.parameters of Linear(in_features=300, out_features=300, bias=False)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dampening': 0,\n",
       "  'lr': 0.1,\n",
       "  'momentum': 0,\n",
       "  'nesterov': False,\n",
       "  'params': [Parameter containing:\n",
       "   -5.7562e-02 -5.1616e-02  3.8042e-02  ...   2.2706e-02 -2.0915e-02 -4.0955e-02\n",
       "   -2.1812e-02 -2.0067e-02  4.5006e-02  ...  -1.6091e-02  1.0634e-02 -3.8151e-02\n",
       "    3.0340e-02 -5.7707e-02  4.9239e-02  ...  -3.3092e-02 -1.0309e-04  3.8321e-02\n",
       "                   ...                   â‹±                   ...                \n",
       "    9.2728e-03  5.2941e-02  5.1151e-02  ...   1.1323e-02 -3.7980e-02 -4.6198e-02\n",
       "   -1.4374e-02 -2.6682e-02 -4.7653e-02  ...   4.3119e-02 -5.5847e-02  1.4095e-02\n",
       "   -3.0719e-02  4.6355e-02 -4.5438e-02  ...  -5.0887e-02  4.9836e-02  2.8728e-02\n",
       "   [torch.cuda.FloatTensor of size 300x300 (GPU 0)]],\n",
       "  'weight_decay': 0}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.map_optimizer.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- ADVERSARIAL TRAINING -------\n",
      "\n",
      "Starting 0 th epoch in adversarial training...\n",
      "000000 - Discriminator loss: 0.6602 - 175 samples/s\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "Using nn for matching pairs\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "('Using CSLS with k = ', 10)\n",
      "Starting 1 th epoch in adversarial training...\n",
      "000000 - Discriminator loss: 0.6099 - 4458 samples/s\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "Using nn for matching pairs\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "('Using CSLS with k = ', 10)\n",
      "Starting 2 th epoch in adversarial training...\n",
      "000000 - Discriminator loss: 0.5698 - 4928 samples/s\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "Using nn for matching pairs\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "('Using CSLS with k = ', 10)\n",
      "Starting 3 th epoch in adversarial training...\n",
      "000000 - Discriminator loss: 0.5310 - 5141 samples/s\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "Using nn for matching pairs\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "('Using CSLS with k = ', 10)\n",
      "Starting 4 th epoch in adversarial training...\n",
      "000000 - Discriminator loss: 0.4953 - 5113 samples/s\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "Using nn for matching pairs\n",
      "Loaded 3 pairs of words in the dictionary (3 unique). 21594 other pairs contained at least one unknown word (21593 in lang1, 21541 in lang2)\n",
      "('Using CSLS with k = ', 10)\n"
     ]
    }
   ],
   "source": [
    "#Adversarial Training\n",
    "print('--------- ADVERSARIAL TRAINING -------\\n')\n",
    "#epoch_size = 1000000\n",
    "for epoch in xrange(param_list.n_epoch_adv):\n",
    "    print('Starting %i th epoch in adversarial training...' % epoch)\n",
    "    tic = time.time()\n",
    "    n_words_proc = 0\n",
    "    stats = {'DIS_COSTS': []}\n",
    "    for n_iter in range(0, param_list.n_epoch_adv, param_list.batch_size_adv):\n",
    "        # discriminator training\n",
    "        for _ in range(param_list.dis_steps_adv):\n",
    "            trainer.dis_step(stats)\n",
    "        # discriminator fooling\n",
    "        n_words_proc += trainer.mapping_step(stats)\n",
    "        #print(stats)\n",
    "        \n",
    "        # log stats\n",
    "        if n_iter % 2000 == 0:\n",
    "            stats_str = [('DIS_COSTS', 'Discriminator loss')]\n",
    "            stats_log = ['%s: %.4f' % (v, np.mean(stats[k]))\n",
    "                         for k, v in stats_str if len(stats[k]) > 0]\n",
    "            stats_log.append('%i samples/s' % int(n_words_proc / (time.time() - tic)))\n",
    "            print(('%06i - ' % n_iter) + ' - '.join(stats_log))\n",
    "\n",
    "            # reset\n",
    "            tic = time.time()\n",
    "            n_words_proc = 0\n",
    "            for k, _ in stats_str:\n",
    "                del stats[k][:]\n",
    "        evaluator.evaluate()\n",
    "\n",
    "    #print stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
